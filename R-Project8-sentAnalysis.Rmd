---
title: "Sentiment Analysis"
author: "Henry Fong, Ian Zhang, Adam Walters"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning  = FALSE}
library(rio)
library(readxl)
library(openxlsx)
library(tm)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(tidyverse)
library(MASS)
library(qdap)
```

```{r}
df <- read_csv("ChatGPT_ResponsesOnAI.csv")

```

```{r}

# Create a corpus
corpG <- Corpus(VectorSource(df$Response))

# Function to remove unknown characters
remove_unknown_characters <- function(text) {
  encoding <- "UTF-8"
  clean_text <- iconv(text, to = encoding, sub = "")
  return(clean_text)
}

# Apply the function to each document in the corpus
corpG <- tm_map(corpG, content_transformer(remove_unknown_characters))

# Further text preprocessing
corpG <- tm_map(corpG, content_transformer(tolower))
corpG <- tm_map(corpG, removeNumbers)
corpG <- tm_map(corpG, removeWords, stopwords("english"))
corpG <- tm_map(corpG, removePunctuation)
corpG <- tm_map(corpG, stripWhitespace)
corpG <- tm_map(corpG, removeWords, c("ai"))

# Stemming
corpG <- tm_map(corpG, stemDocument)

# Sentiment analysis
p2 <- polarity(corpG, group = df$Number,
               polarity.frame = key.pol, negators = negation.words,
               amplifiers = amplification.words,
               deamplifiers = deamplification.words,
               n.before = 2, n.after = 2)

# Handle NaN values
p2$all$polarity[is.nan(p2$all$polarity)] <- 0

# K-means clustering
```

```{r}

```
